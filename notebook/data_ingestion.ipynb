{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\t\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What must be the columns of the DF \n",
    "# Title, Article, category, Link, Source, Author(optional), Date\n",
    "# Title[string]: Short catchy gist of the article\n",
    "# Article[string]: Main content\n",
    "# Category[string]: the type of article\n",
    "# Link[string]: Href of the article\n",
    "# Source[string]: Name of the news site\n",
    "# Author[string]: Name of the writer\n",
    "# Date[Date]: Date of publishing\n",
    "\n",
    "# What must I do to correct the previously collected data?\n",
    "# Well good thing is I almost have the sources, If I somehow manage to get links then date and author are easy. \n",
    "# However, getting the link of the article is a mystery of it's own. There is no direct path for getting a link. \n",
    "# I scraped what I found on the front page that doesn't mean it's going to be there all the time. \n",
    "# I'll have to think about that and clean the data as much as it is possible before manually labelling it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels of the Data\n",
    "# Political => 0: rightist; 0.5: centrist or neutral; 1: leftist\n",
    "# framing(traditional) => 0: negative; 0.5: neutral; 1: positive\n",
    "# sensationalism => 0: No exageration; 1: highly exagerated\n",
    "# opinionated(selection bias, and stereotyping) => 0: low to none opinionated or facts; 1: highly opinionated\n",
    "\n",
    "# There are 4 ouput classes with independent degree of reliability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbc_articles():\n",
    "    categories_to_scrape = [\n",
    "        \"https://www.bbc.com/news/world/asia\", \n",
    "        \"https://www.bbc.com/business\", \n",
    "        \"https://www.bbc.com/news\", \n",
    "        \"https://www.bbc.com/innovation\"\n",
    "    ]\n",
    "    \n",
    "    articles_links = []\n",
    "    for category in categories_to_scrape:\n",
    "        cat_res = requests.get(category)\n",
    "        cat_soup = BeautifulSoup(cat_res.content, \"html.parser\")\n",
    "        \n",
    "        for link in cat_soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if href.startswith(\"/news/articles/\") and href not in articles_links:\n",
    "                articles_links.append([\"https://www.bbc.com\" + href, category[20:]])\n",
    "    \n",
    "    urls_to_scrape = articles_links\n",
    "    articles = []\n",
    "    categories = []\n",
    "    titles = []\n",
    "    links = []\n",
    "    sources = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "    \n",
    "    for url in urls_to_scrape:\n",
    "        response = requests.get(url=url[0])\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        \n",
    "        text = \"\"\n",
    "        body = soup.body\n",
    "        \n",
    "        # Extract heading\n",
    "        heading_tag = body.find(\"h1\", class_=\"sc-518485e5-0 itISwu\")\n",
    "        heading = heading_tag.get_text(strip=True) if heading_tag else \"Unknown Title\"\n",
    "        \n",
    "        # Extract article text\n",
    "        paragraphs = body.find('article').find_all(\"p\", class_=\"sc-eb7bd5f6-0 fezwLZ\")\n",
    "        for para in paragraphs:\n",
    "            text += para.get_text() + \" \"\n",
    "        \n",
    "        # Extract author (optional)\n",
    "        author_tag = body.find_all(\"span\", class_=\"sc-b42e7a8f-7 kItaYD\") \n",
    "        authors_ = []\n",
    "        for auth in author_tag:\n",
    "            authors_.append(auth.get_text())\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = body.find(\"time\")\n",
    "        date = date_tag.get(\"datetime\") if date_tag else \"Unknown Date\"\n",
    "        \n",
    "        # Store extracted data\n",
    "        articles.append(text)\n",
    "        categories.append(url[1])\n",
    "        titles.append(heading)\n",
    "        authors.append(', '.join(authors_))\n",
    "        links.append(url[0])\n",
    "        sources.append(\"BBC News\")\n",
    "        dates.append(date)\n",
    "    \n",
    "    return articles, categories, titles, links, sources, authors, dates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mint_articles():\n",
    "    categories_to_scrape = [\n",
    "        \"https://www.livemint.com/news/india\",\n",
    "        \"https://www.livemint.com/news/world\",\n",
    "    ]\n",
    "    article_links = []\n",
    "\n",
    "    for category in categories_to_scrape:\n",
    "        cat_res = requests.get(category)\n",
    "        cat_soup = BeautifulSoup(cat_res.content, \"html.parser\")\n",
    "        for link in cat_soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if href.startswith(\"/news/\") and href not in article_links:\n",
    "                article_links.append([\"https://www.livemint.com\" + href, f\"{category[25:]}\"])\n",
    "\n",
    "    urls_to_scrape = article_links\n",
    "    articles = []\n",
    "    cats = []\n",
    "    titles = []\n",
    "    links = []\n",
    "    sources = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "\n",
    "    try:\n",
    "        for url in urls_to_scrape:\n",
    "            response = requests.get(url=url[0])\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            text = \"\"\n",
    "            body = soup.body\n",
    "            \n",
    "            if not body:\n",
    "                pass\n",
    "            else:\n",
    "                # Extract title\n",
    "                heading = body.find('h1', id=\"article-0\")\n",
    "                title_text = heading.get_text(strip=True) if heading else \"Unknown Title\"\n",
    "\n",
    "                # Extract article text\n",
    "                vals = body.find(\"div\", class_=\"storyPage_storyContent__m_MYl\")\n",
    "                if vals:\n",
    "                    paragraph_section = vals.find(\"div\", class_=\"storyParagraph\")\n",
    "                    if paragraph_section:\n",
    "                        paragraphs = paragraph_section.find_all(\"p\")\n",
    "                        for val in paragraphs:\n",
    "                            text += val.get_text()\n",
    "\n",
    "                # Extract author\n",
    "                author_tag = body.find(\"div\", class_=\"storyPage_authorDesc__zPjwo\")\n",
    "                \n",
    "                author_text = author_tag.find('a').get_text(strip=True) if  author_tag and author_tag.find('a') else \"Unknown\"\n",
    "\n",
    "                # Extract date\n",
    "                date_text = author_tag.find('span').get_text(strip=True) if author_tag else \"Unknown Date\"\n",
    "\n",
    "                # Append extracted data\n",
    "                articles.append(text)\n",
    "                cats.append(url[1])\n",
    "                titles.append(title_text)\n",
    "                links.append(url[0])\n",
    "                sources.append(\"Mint\")\n",
    "                authors.append(author_text)\n",
    "                dates.append(date_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        raise e\n",
    "\n",
    "    return articles, cats, titles, links, sources, authors, dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ie_articles():\n",
    "    categories_to_scrape = [\n",
    "        \"https://indianexpress.com/section/india/\",\n",
    "        \"https://indianexpress.com/section/world/\",\n",
    "    ]\n",
    "    article_links = []\n",
    "\n",
    "    for category in categories_to_scrape:\n",
    "        cat_res = requests.get(category)\n",
    "        cat_soup = BeautifulSoup(cat_res.content, \"html.parser\")\n",
    "        for link in cat_soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if \"/article/\" in href and href not in article_links:\n",
    "                article_links.append([href, f\"{category[26:]}\"])\n",
    "\n",
    "    urls_to_scrape = article_links\n",
    "    articles = []\n",
    "    cats = []\n",
    "    titles = []\n",
    "    links = []\n",
    "    sources = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "\n",
    "    try:\n",
    "        for url in urls_to_scrape:\n",
    "            response = requests.get(url=url[0])\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            text = \"\"\n",
    "            body = soup.body\n",
    "\n",
    "            # Extract title\n",
    "            heading = body.find('h1', id=\"main-heading-article\")\n",
    "            title_text = heading.get_text(strip=True) if heading else \"Unknown Title\"\n",
    "\n",
    "            # Extract article text\n",
    "            vals = body.find(\"div\", class_=\"story_details\")\n",
    "            if vals:\n",
    "                paragraphs = vals.find_all(\"p\")\n",
    "                for val in paragraphs:\n",
    "                    text += val.get_text()\n",
    "\n",
    "            # Extract author\n",
    "            author_tag = body.find(\"div\", id=\"storycenterbyline\")\n",
    "            author_text = author_tag.find('a').get_text(strip=True) if author_tag and author_tag.find('a') else \"Unknown\"\n",
    "\n",
    "            # Extract date\n",
    "            date_tag = body.find(\"span\", itemprop=\"dateModified\")\n",
    "            date_text = date_tag['content'] if date_tag['content'] else \"Unknown Date\"\n",
    "\n",
    "            # Append extracted data\n",
    "            articles.append(text)\n",
    "            cats.append(url[1])\n",
    "            titles.append(title_text)\n",
    "            links.append(url[0])\n",
    "            sources.append(\"Indian Express\")\n",
    "            authors.append(author_text)\n",
    "            dates.append(date_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        raise e\n",
    "\n",
    "    return articles, cats, titles, links, sources, authors, dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_it_articles():\n",
    "    categories_to_scrape = [\n",
    "        \"https://www.indiatoday.in/world/\",\n",
    "        \"https://www.indiatoday.in/india/\",\n",
    "    ]\n",
    "    article_links = []\n",
    "\n",
    "    for category in categories_to_scrape:\n",
    "        cat_res = requests.get(category)\n",
    "        cat_soup = BeautifulSoup(cat_res.content, \"html.parser\")\n",
    "        for link in cat_soup.find_all(\"a\", href=True):\n",
    "            href = link[\"href\"]\n",
    "            if (\n",
    "                href.startswith(\"/world/us-news/\") or href.startswith(\"/world/uk-news/\") or href.startswith(\"/india/\")\n",
    "            ) and ('video' not in href) and href not in article_links:\n",
    "                article_links.append(\n",
    "                    [\"https://www.indiatoday.in\" + href, f\"{category[25:]}\"]\n",
    "                )\n",
    "\n",
    "    urls_to_scrape = article_links\n",
    "    articles = []\n",
    "    cats = []\n",
    "    titles = []\n",
    "    links = []\n",
    "    sources = []\n",
    "    authors = []\n",
    "    dates = []\n",
    "\n",
    "    try:\n",
    "        for url in urls_to_scrape:\n",
    "            response = requests.get(url=url[0])\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            text = \"\"\n",
    "            body = soup.body\n",
    "\n",
    "            # Extract title\n",
    "            heading = body.find(\n",
    "                \"h1\", class_=\"jsx-ace90f4eca22afc7 Story_strytitle__MYXmR\"\n",
    "            )\n",
    "            title_text = heading.get_text(strip=True) if heading else \"Unknown Title\"\n",
    "\n",
    "            # Extract article text\n",
    "            content_div = body.find(\n",
    "                \"div\",\n",
    "                class_=\"jsx-ace90f4eca22afc7 Story_description__fq_4S description paywall\",\n",
    "            )\n",
    "            if content_div:\n",
    "                paragraphs = content_div.find_all(\"p\")\n",
    "                for para in paragraphs:\n",
    "                    text += para.get_text()\n",
    "\n",
    "            # Extract author\n",
    "            author_tag = body.find(\"div\", class_=\"authors__by\")\n",
    "            author_text = (\n",
    "                author_tag.find(\"div\", class_=\"authdetaisl\").get_text(strip=True)\n",
    "                if author_tag and author_tag.find(\"div\", class_=\"authdetaisl\")\n",
    "                else \"Unknown\"\n",
    "            )\n",
    "\n",
    "            # Extract date\n",
    "            date_tag = body.find(\"div\", class_=\"published__on\")\n",
    "            date_text = date_tag.find('div', class_='authdetaisl').get_text(strip=True) if date_tag and date_tag.find(\"div\", class_=\"authdetaisl\") else \"Unknown\"\n",
    "\n",
    "            # Append extracted data\n",
    "            articles.append(text)\n",
    "            cats.append(url[1])\n",
    "            titles.append(title_text)\n",
    "            links.append(url[0])\n",
    "            sources.append(\"India Today\")\n",
    "            authors.append(author_text)\n",
    "            dates.append(date_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(url)\n",
    "        raise e\n",
    "\n",
    "    return articles, cats, titles, links, sources, authors, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_object():\n",
    "\tarticles_bbc, category_bbc, title_bbc, links_bbc, sources_bbc, authors_bbc, dates_bbc = get_bbc_articles()\n",
    "\tarticle_mint, category_mint, title_mint, links_mint, sources_mint, authors_mint, dates_mint = get_mint_articles()\n",
    "\tarticle_ie, category_ie, title_ie, links_ie, sources_ie, authors_ie, dates_ie = get_ie_articles()\n",
    "\tarticle_it, category_it, title_it, links_it, sources_it, authors_it, dates_it = get_it_articles()\n",
    "\tarticles = articles_bbc + article_mint + article_ie + article_it\n",
    "\tcategory = category_bbc + category_mint + category_ie + category_it\n",
    "\ttitle = title_bbc + title_mint + title_ie + title_it\n",
    "\tlinks = links_bbc + links_mint + links_ie + links_it\n",
    "\tsources = sources_bbc + sources_mint + sources_ie + sources_it\n",
    "\tauthors = authors_bbc + authors_mint + authors_ie + authors_it\n",
    "\tdates  = dates_bbc + dates_mint + dates_ie + dates_it\n",
    "\tdata = pd.DataFrame({ \"title\": title, \"article\": articles, \"category\": category, 'links': links, 'sources': sources, 'authors': authors, 'dates': dates})\n",
    "\tdata = data.drop_duplicates()\n",
    "\treturn data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data_csv(data):\n",
    "\tname = datetime.today().strftime('%Y-%m-%d_%H-%M')\n",
    "\tdata.to_csv(f\"../data/all_artice_{name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(file_path):\n",
    "\treturn pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data = make_data_object()\n",
    "latest_data.drop_duplicates(subset=['title'], keep='first', inplace=True)\n",
    "latest_data = latest_data.reset_index()\n",
    "latest_data.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       206\n",
       "article     206\n",
       "category    206\n",
       "links       206\n",
       "sources     206\n",
       "authors     206\n",
       "dates       206\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_art = pd.read_csv(\"../data/Usecase/all_labelle-articles.csv\")\n",
    "lab_art.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             656\n",
       "article           656\n",
       "category          656\n",
       "links             656\n",
       "sources           656\n",
       "authors           652\n",
       "dates             656\n",
       "political         482\n",
       "framing           482\n",
       "sensationalism    482\n",
       "opinionated       482\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlab_art = pd.read_csv(\"../data/Usecase/all_unlabelled_articles.csv\")\n",
    "unlab_art.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([lab_art, unlab_art, latest_data], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=['title'], keep='first')\n",
    "data = data.reset_index()\n",
    "data.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title             836\n",
       "article           836\n",
       "category          836\n",
       "links             836\n",
       "sources           836\n",
       "authors           832\n",
       "dates             836\n",
       "political         482\n",
       "framing           482\n",
       "sensationalism    482\n",
       "opinionated       482\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_csv(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
